<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Interactive Kalman Filter</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>

    <script src="js/draggable-number.js"></script>
    <script src="js/gaussian-functions.js"></script>
    <script src="js/formula-elements.js"></script>
    <script src="js/mouse-effects-manager.js"></script>
    <script>
        const mouseEffectsManager = new MouseEffectsManager([...Object.values(variables)]);
    </script>
    <script>
        MathJax = {
            //Enable macros \class, \cssId, \href, \style
            loader: {load: ['[tex]/html']}, 
            tex: {packages: {'[+]': ['html']}},
            // save custom TexMacros in the MathJax configuration object 
            // (becomes MathJax.config.TexMacros after MathJax is loaded)
            TexMacros: {
                // Note: use two '\'s because this is first intepreted as a javascript strings, where '\' needs to be escaped
                wikipedia: `
                \\def\\state{\\class{state}{\\mathbf{\\bar{x}}}}
                \\def\\stateEstimate{\\class{state-estimate}{\\mathbf{x}}}
                \\def\\stateTransition{\\class{state-transition}{\\mathbf{F}}}
                \\def\\controlInput{\\class{control-input}{\\mathbf{u}}}
                \\def\\measurement{\\class{measurement}{\\mathbf{z}}}
                \\def\\measurementNoiseCovariance{\\class{measurement-noise-covariance}{\\mathbf{R}}}
                \\def\\measurementMatrix{\\class{measurement-matrix}{\\mathbf{H}}}
                \\def\\stateCovariance{\\class{state-covariance}{\\mathbf{P}}}
                \\def\\processNoiseCovariance{\\class{process-noise-covariance}{\\mathbf{Q}}}
                \\def\\controlMatrix{\\class{control-matrix}{\\mathbf{B}}}
                \\def\\kalmanGain{\\class{kalman-gain}{\\mathbf{K}}}
                \\def\\varianceRatio{\\class{variance-ratio}{\\widetilde{\\mathbf{K}}_{z}}}
                \\def\\varianceRatioState{\\class{variance-ratio-state}{\\widetilde{\\mathbf{K}}_{x}}}
                \\def\\discreteTime{n}
                \\def\\identityMatrix{\\class{identity-matrix}{\\mathbf{I}}}
                \\def\\gaussian{\\class{gaussian-distribution}{\\mathcal{N}}}
                `,
                kalmanfilterdotnet: `
                \\def\\state{\\class{state}{\\mathbf{\\bar{s}}}}
                \\def\\stateEstimate{\\class{state-estimate}{\\mathbf{s}}}
                \\def\\stateTransition{\\class{state-transition}{\\mathbf{A}}}
                \\def\\controlInput{\\class{control-input}{\\mathbf{u}}}
                \\def\\measurement{\\class{measurement}{\\mathbf{z}}}
                \\def\\measurementNoiseCovariance{\\class{measurement-noise-covariance}{\\mathbf{R}}}
                \\def\\measurementMatrix{\\class{measurement-matrix}{\\mathbf{C}}}
                \\def\\stateCovariance{\\class{state-covariance}{\\mathbf{P}}}
                \\def\\processNoiseCovariance{\\class{process-noise-covariance}{\\mathbf{Q}}}
                \\def\\controlMatrix{\\class{control-matrix}{\\mathbf{B}}}
                \\def\\kalmanGain{\\class{kalman-gain}{\\mathbf{K}}}
                \\def\\varianceRatio{\\class{variance-ratio}{\\widetilde{\\mathbf{K}}_{z}}}
                \\def\\varianceRatioState{\\class{variance-ratio-state}{\\widetilde{\\mathbf{K}}_{x}}}
                \\def\\discreteTime{t}
                \\def\\identityMatrix{\\class{identity-matrix}{\\mathbf{I}}}
                \\def\\gaussian{\\class{gaussian-distribution}{\\mathcal{N}}}
                `
            },
          // Custom function to get the desired set of macros,
          // then have the renderer to go back to the compile step so the new macros get used.
          setMacros(name) {
            if (!MathJax.config.TexMacros.hasOwnProperty(name)) {
              console.warn(`TexMacros for "${name}" not found.`);
              return Promise.resolve();
            }
            
            return MathJax.startup.promise = MathJax.startup.promise.then(() => {
              const {mathjax} = MathJax._.mathjax;
              const {STATE} = MathJax._.core.MathItem;
              MathJax.tex2mml(MathJax.config.TexMacros[name]);
              mathjax.handleRetriesFor(() => MathJax.startup.document.rerender(STATE.COMPILED));
              
              mouseEffectsManager.initialize();
            });
          },
          startup: {
            typeset: false, //disable the initial typesetting pass since setMacros() will do typesetting
            ready() {
              MathJax.startup.defaultReady();
              MathJax.config.setMacros('wikipedia');
              const dropdown = document.getElementById('naming-convention-dropdown');
              if (!dropdown) {
                console.warn('MathJax ready(): No dropdown element found. Not changing from the default.');
                return;
              }
              dropdown.addEventListener('input', (event) => MathJax.config.setMacros(event.target.value));
            }
          }
        }
    </script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        /* Formatting for the entire page */
        body {
            font-family: serif;
            margin: 40px auto;
            /* padding: 40px; This made the text adjust too late */
            background-color: hsl(40, 100%, 98%);
            color: hsl(208, 30%, 18%);
            max-width: 45rem; /* Adjust this value as needed for desired width */
            width: 100%;
        }

        h1, h2 {
            font-family: sans-serif;
        }

        .matrix-table {
            position: relative;
        }
        /* Rectangular pseudo-element before and after the table */
        .matrix-table::before,
        .matrix-table::after {
            content: "";
            position: absolute;
            top: 0;
            bottom: 0;
            width: 4px; 
        }
        /* Make the left pseudo-element look like a bracket */
        .matrix-table::before {
            left: 0px;
            border-left: 2px solid black;
            border-top: 2px solid black;
            border-bottom: 2px solid black;
        }
        /* Make the right pseudo-element look like a bracket */
        .matrix-table::after {
            right: 0px;
            border-right: 2px solid black;
            border-top: 2px solid black;
            border-bottom: 2px solid black;
        }
        .matrix-table td {
            text-align: center;
            vertical-align: middle;
            width: 6ch; /* matches the DraggableNumber width */
        }
        .matrix-table tr {
            line-height: 1.5;
        }

        .math-diagram-structure-table {
            border-collapse: collapse;
            margin-left: auto;
            margin-right: auto;
        }

        .math-diagram-structure-table td {
            text-align: center;
            vertical-align: middle;
        }

        .kalman-gain-state-space {
            background-color: rgb(187, 187, 187);
        }

        .kalman-gain-intermediate-space {
            background-color: #dddddd;
        }

        .kalman-gain-measurement-space {
            background-color: rgb(255, 252, 238);
        }

        #kalman-gain-Hstate-gaussian {
            border: 2px solid rgb(0, 0, 255);
            border-radius: 10px;
        }

        #kalman-gain-measurement-gaussian {
            border: 2px solid rgb(255, 0, 0);
            border-radius: 10px;
        }

        #kalman-gain-combined-gaussian {
            border: 2px solid rgb(0, 150, 0);
            border-radius: 10px;
        }

        /* Outline the table temporarily so I can see what I'm doing */

        /* .math-diagram-structure-table {
            border: 1px solid #ddd;
        } */

        /* .math-diagram-structure-table td {
            border: 1px solid #ddd;
        }

        .math-diagram-structure-table tr:nth-child(even) {
            background-color: #f2f2f2;
        } */

    </style>
</head>

<body>
    <select id="naming-convention-dropdown">
        <option value="wikipedia">Wikipedia</option>
        <option value="kalmanfilterdotnet">KalmanFilter.net</option>
    </select>

    <h1>The Interactive Kalman Filter</h1>

    <h2>Introduction</h2>
    <p>The Kalman Filter is a powerful tool for combining information in the face of uncertainty,
        and is used widely throughout science and engineering for tracking dynamic systems. 
        Unfortunately, most of the explanations you'll find are either mathematically dense to 
        the point of being impenetrable, and/or leave a lot of questions remaining as to why we 
        intuitively should expect the equations to look as they do. In this article, I hope to 
        teach you how deep principles in probability and linear algebra (which you probably 
        already know!) naturally lead to the equations of the Kalman Filter. I've also made 
        a series of interactive diagrams that will hopefully give you a better 
        feel for the different parameters in all of their intricacy, and understand both the power and limitations
        of this method.
    </p>

    <h3>What is the Kalman Filter?</h3>
    <p>
    The primary goal of the Kalman Filter is to estimate the state of a dynamical system.
    The need for state estimation shows up throughout engineering: if you're trying to drive a car, you first need to know where you
    are and how fast you're going before you can intelligently turn the steering wheel.
    If you're controlling the central heating in a building, it's very helpful to know
    the temperature in the rooms. The challenge is that although we often have many sensors,
    they are usually noisy and cannot individually perceive the full state. These sensors need 
    to be combined with each other and with the engineer's knowledge of the system in order 
    to get a complete picture.

    The Kalman Filter is a theoretically optimal way to combine different sources of information into one unified state estimate. 

    </p>

    <h2>Building a model</h2>
    <p>The easiest way to wrap your head around the Kalman Filter is with an example.
        Throughout this artile, we'll use this simple robot.
    </p>
    <p>We begin with a large question: How to model the world? A common engineering approach is 
        to think of the minimum set of variables needed to make a complete description of the system, 
        and then put them all together in a state vector. In our simple example, all we need is the position \(p\) 
        and velocity \(v\) of the robot. 
    </p>
    <p>TODO: put in popout box:     In general, the state vector could include many different quantities: in
        Fluid dynamics it would be things like temperature, pressure, flow rate,
        volume, and in a population dynamics model it might include the number of
        individuals and their age distribution.
    </p>
    <p>$$ \text{State vector }\stateEstimate = \begin{bmatrix} p \\ v \end{bmatrix} 
            = \begin{bmatrix} \text{position} \\ \text{velocity} \end{bmatrix} $$
    </p>
    <p> Once we've defined our state vector, we need to model how it evolves over time.
    For our robot, we can use a simple kinematics formula:
    $$ \begin{alignat}{3}
        p_{\discreteTime} &= p_{\discreteTime-1} &+& \Delta t &v_{\discreteTime-1} \\
        v_{\discreteTime} &= 0                   &+&          &v_{\discreteTime-1} \\
        \end{alignat}
    $$
    </p>
    <p>We'll package this up into matrix form so it is easy to use later.
        We'll call the matrix that converts from the old state to a new state 
        using the dynamics model the state transition matrix \(\stateTransition\).

        $$ \text{State Transition Matrix } \stateTransition = \begin{bmatrix} 1 & \Delta t \\ 0 & 1 \end{bmatrix} $$
        $$
        \begin{alignat}{3}
            \begin{bmatrix}
                p_{\discreteTime} \\
                v_{\discreteTime}
            \end{bmatrix}
            &=
            \begin{bmatrix}
                1 & \Delta t \\
                0 & 1
            \end{bmatrix}
            &
            \begin{bmatrix} 
                p_{\discreteTime-1} \\ 
                v_{\discreteTime-1} 
            \end{bmatrix} \\

            \stateEstimate_{\discreteTime} &= \stateTransition & \stateEstimate_{\discreteTime-1} \\
        \end{alignat}
        $$
    </p>

    <p>
        Now we have a simple dynamical model of our robot, but it fails to include one
        thing that is common to most robots: a way to control it. In the demo above, you
        (as the operator) are able to control the robot with a throttle command \(u\),
        which can range between [0,1]. Inherently this [0,1] value has no meaning, but
        the robot is programmed so that the throttle controls the angular speed of
        the wheels, which in turn determines the robot's linear velocity.
        If we assume that the has radius \(r\), the maximum angular speed
        is \(\omega_{max} \) (and that the wheel speed perfectly matches the control 
        command), then we have the following relationship between the throttle command \(u\) and the velocity \(v\) of the robot:

        $$ v = r \omega = r \omega_{max} u $$

        As with the state equation, we'll package this up in matrix form by defining
        the control matrix \(\controlMatrix\), which determines how the input 
        vector \(\controlInput \) impacts the state. Note that the throttle
        has no effect on the position, so the first row of \(\controlMatrix\) is zero.

        $$ \text{Control Input Vector } \controlInput = \begin{bmatrix} u \end{bmatrix} $$

        $$ \text{Control Matrix } \controlMatrix = \begin{bmatrix} 0 \\ r \omega_{max} \end{bmatrix} $$
        
        Combining this with our dynamics model, our complete dynamical model looks like this:

        $$
        \begin{alignat}{5}
            \begin{bmatrix}
                p_{\discreteTime} \\
                v_{\discreteTime}
            \end{bmatrix}
            &=
            \begin{bmatrix}
                1 & \Delta t \\
                0 & 1
            \end{bmatrix}
            &
            \begin{bmatrix} 
                p_{\discreteTime-1} \\ 
                v_{\discreteTime-1} 
            \end{bmatrix}
            &+
            \begin{bmatrix}
                0 \\
                r \omega_{max} \\
            \end{bmatrix}
            &
            \begin{bmatrix}
                u_{\discreteTime} \\
            \end{bmatrix} \\
            \stateEstimate_{\discreteTime} &= \stateTransition & \stateEstimate_{\discreteTime-1} &+ \controlMatrix & \controlInput_{\discreteTime} \\
        \end{alignat}
        $$

        In summary, we <it>predict</it> the new state by applying the dynamics model to the previous state estimate, and adding 
        a correction term for known influences like the control input. 
    </p>
    <h2>Bringing Uncertainty into the Model</h2>
    <p>
        The linear model we derived above is great and all, but let's be honest, it's not very realistic.
        In the real world, the robot might slip and bounce over the terrain, the motors might not
        exactly reach the commanded speed. In other words, there is random noise introduced by the 
        messiness of the environment, which in turn makes our state estimate less certain.
    </p>
    <p>
        One method that can represent both the uncertainty in our state estimate 
        and the random noise introduced by the environment is to assign each of them a 
        probability distribution. For simplicity, we assume that both of these distributions are gaussian.
    </p>

    <p> TODO: Put this in a popout box
        Why a gaussian? There are a few really compelling reasons: 
        <ol>
            <li>We can parametrize the distribution with just the mean and variance, so it is numerically easy to update</li>
            <li>When we combine two independent gaussians into one estimate, the result is also a gaussian</li>
            <li>Mathematicians have proven (the Central Limit Theorem)that the combined effect of many random processes (even if individually they are non-gaussian)
                will result in a gaussian distribution</li>
        </ol>

        Take heed: we choose a gaussian mostly because it is numerically convenient, but in practice many things (even something as simple
        as a \( sin \theta \) term in the dynamics model introduced by turning around), will make the distribution non-gaussian.
    </p>
    <p> So now, instead of representing our state estimate with a single vector, we'll say that our state estimate 
        is a normal distribution parametrized by a mean \(\state\) and a covariance \(\stateCovariance\).
        $$ \stateEstimate \sim \gaussian \left( \state, \stateCovariance \right) $$
    </p>
    <p>
        Hopefully you've already seen a 1D normal distribution
    </p>
    
    <p> TODO: Diagram: Play around with a single gaussian: Covariance, mean</p>

    <p>Now comes an important point: In our dynamics equation above we know how to apply the matrix \(\stateTransition\) 
        to the state estimate \( \stateEstimate \) when it was a single vector. But now our estimate is a gaussian,
        so we need to apply \(\stateTransition\) to the whole distribution. Fortunately, the mean is a single point,
        so we can directly apply the state transition matrix as we did before. The variance on the other hand is the 
        average "squared-distance" from the mean, so how should it be treated?
    </p>
    <p> We'll introduce a very useful rule:
        In order to multiply a gaussian distribution \(\gaussian \left( \mathbf{\bar{\mu}}, \mathbf{\Sigma} \right) \) by a matrix \( \mathbf{A} \), 
        the mean is simply multiplied normally, while the covariance is multiplied by \( \mathbf{A} \) squared.
    </p>
    <p>
        Linear Transformation of a Normal Distribution:
        $$
        \mathbf{\bar{\mu}}_{\discreteTime} = \mathbf{A} \mathbf{\bar{\mu}}_{\discreteTime-1} 
        $$
        $$
        \mathbf{\Sigma}_{\discreteTime} = \mathbf{A} \mathbf{\Sigma}_{\discreteTime-1} \mathbf{A}^T
        $$
    </p>
    <p>
        One way you can remember this is to think back to what happens when you multiply a 1D normal distribution by a scalar.
        The standard deviation \( \sigma \) will scale linearly, and the variance is the standard deviation squared
        (which we can write in a way that is suggestive of what the operation looks like in higher dimensions).
        <div style="max-width: 100%; text-align: center;">
            <img src="./assets/variance-diagram-1d.png" alt="Diagram showing the effect of multiplying a 1D Gaussian by a scalar" style="max-width: 100%; height: auto;">
        </div>
    </p>

    <p>With this probabilistic knowledge, we have actually arrived to the point where we can write out the first component of the Kalman Filter:
        the <strong>Predict Step</strong>! We'll define the covariance of the state estimate to be \(\stateCovariance\), and the mean to be \(\state\). 
        To handle environmental noise, we'll also add a noise term with mean 0 and covariance \(\processNoiseCovariance\). Writing this out, we have:
    </p>
    <p>The Predict Step:
        $$ \state_{\discreteTime} = \stateTransition \state_{\discreteTime-1} + \controlMatrix \controlInput_{\discreteTime} $$
        $$ \stateCovariance_{\discreteTime} = \stateTransition \stateCovariance_{\discreteTime-1} \stateTransition^T + \processNoiseCovariance $$
    where
    $$ \stateEstimate \sim \gaussian \left( \state, \stateCovariance \right) $$
    and $$ \text{process noise} \sim \gaussian \left( 0, \processNoiseCovariance \right) $$
    </p>
    <p> 
        Fundamentally, the predict step describes how to <it>predict</it> the a new gaussian state estimate from the previous estimate, 
        based on our dynamical model of the system. Let's try running this algorithm in state space: 
    </p>

    <p> TODO: Diagram: Predict Step State Space diagram</p>
    
    <h2>Sensing the environment</h2>
    <p>Ok so the state ... isn't terrible, but it's not great either, and the variance is exploding (TODO: Popout box explaining why this makes sense)..
        Fortunately, we have endowed this robot with an ultrasonic distance sensor that should help.         
        The only question is, how do we incorporate these measurements into the filter? 
    </p>
    <p>
        To start with, we need a linear model that relates the measurement vector \( \measurement \) to the state \( \trueState \).
        Our first instinct might be to write a matrix that converts from the measurement vector to the state vector 
        (we're trying to update the state after all). The problem is 
        that since the sensor doesn't measure the velocity, there is no way to fully recover the state from the measurement 
        using a linear transformation. We can however think about this from a more causal perspective: 
    </p>
    <p>
        In reality we know that the robot is in some true state x, and when the sensor takes a measurement
        of that state it will generally follow a linear process. If we define the measurement matrix \(\measurementMatrix\)
        to describe this linear measurement process, then in the ideal case we can write:
        $$ \measurement_{ideal} = \measurementMatrix \trueState $$
    </p>
    <p>Let's get into some implementation details to see what \(\measurementMatrix\) looks like. 
        Most ultrasonic sensors do not directly provide a distance. Instead, they measure 
        the time (in microseconds, (\ \mu s \)) it takes for a pulse of sound to travel to the wall and back. In order to convert from (\ \mu s \)
        to meters, we need to multiply by a factor of \( \frac{2 \times 10^6 }{c} \), where \(c\) is the speed of sound in air (343 m/s), 
        there are \( 10^6 \mu s\) per second, and the factor of 2 accounts for the sound traveling double the distance (to the wall, and then back).
        Thus to convert from the measurement vector to the state vector, the measurement matrix would be:
        $$ \text{Measurement Matrix} \measurementMatrix = \begin{bmatrix} \frac{2 \times 10^6 }{c} & 0 \end{bmatrix} $$
    </p>
    <p>Of course, no sensor is ever perfect, so we will also say that when the sensor takes a measurement 
        it will also introduce some sensor noise. We assume that the noise is normally distributed with 
        zero mean and covariance \(\measurementNoiseCovariance\) (which hopefully is listed in the sensor datasheet).
        $$ \text{Measurement Noise} \sim \gaussian \left( 0, \measurementNoiseCovariance \right) $$
        
        Now we can say that when the robot is in state x, the measurements you can expect to get will
        follow a normal distribution with covariance \(\measurementNoiseCovariance\) and mean \(\measurementMatrix \trueState\).
        Of course, we never actually see the entirety of this distribution; the robot will only ever take 
        individual measurements. Since \( \measurement \) is the only sample we have, 
        we will say that every time we read some measurement, we infer that the distribution of possible 
        measurements is a gaussian centered at \( \measurement \) with covariance \(\measurementNoiseCovariance\):
        $$ \measurement \sim \gaussian \left( \measurementMatrix \trueState, \measurementNoiseCovariance \right) $$
    </p>

    <p> To summarize, we've defined the measurement matrix \( \measurementMatrix \), whose role is to 
        convert from the space of states to the space of measurements. \( \measurementMatrix \) can be 
        applied to the true state to get the noiseless measurement, but it can also be applied to 
        the entire distribution of our current state estimate, which yields the measurement we expect given 
        the current state estimate. 
    </p>
    
    <h2>Update Step: Combining two gaussians</h2>
    <p>
        At this point, we have one normal distribution that represents our belief of the true state of the robot,
        and another normal distribution that represents the measurements you'd expect to get from the true state. 
        How can we combine these two distributions into one new state estimate? 
    </p>
    <p>The first challenge we face is that we're trying to combine different "types" of things.
        The state estimate distribution has a mean and variance with a particular dimension and units,
        while the measurement distribution has entirely different dimensions and units. Before
        we can do any combining, we have to first get both distributions into the same space. 
    </p>
    <p>There are two directions we could take in combining the measurement distribution with the state estimate distribution. 
        Click on the one you want to follow!
    </p>
    <p>Option 1: See what measurement we should expect given our current state estimate, compare this 
        with the actual measurement we got, then convert a "fused measurement" back to state space.
    </p>

    <p>In order to put our two distributions on the same footing, we'll multiply the
        state estimate distribution by \( \measurementMatrix \) to bring it into 
        to the space of measurements. Remembering our rule of thumb, when we multiply the mean by \( \measurementMatrix \),
        we have to multiply the variance by \( \measurementMatrix \) on the left and right.
        This will leave us with two normal distributions in the same space:

        <ul>
            <li>The measurement you'd expect at the state you Predicted: \( \gaussian \left( \measurementMatrix \state, \space \measurementMatrix \stateCovariance \measurementMatrix^T \right) \)</li>
            <li>The measurement you actually got: \( \gaussian \left( \measurement, \space \measurementNoiseCovariance \right) \)</li>
        </ul>
    </p>

    <p>Now that we have two distributions in the same space, we can stop to think: what would we like to 
        be true when we combine these estimates? Here are some things that would be nice:
    </p>
    <ol>
        <li>The mean of the combined estimate should always lie between the means of the two original estimates</li>
        <li>The uncertainty (ie. variance) should be smaller than the uncertainty of either of the original estimates</li>
        <li>The resulting estimate should more closely resemble the distribution in which we have higher confidence (ie. less variance).</li>
    </ol>
    
    <p>For example, if we know that the ultrasonic sensor is much more accurate than our dynamics model (which is propagated blindly), then 
        we'd want our combined estimate to consist mostly of the sensor measurement, with a hint of the dynamics model 
        to account for small measurement noise. Since we're making use of all of our information, we should 
        be more certain of our combined estimate than either of the original estimates independently.
    </p>
    <p>But how much, exactly, of each estimate do we want to use? One reasonable approach is to weight each distribution 
        by the inverse of its variance, so the distribution with less uncertainty (ie. smaller variance) will play more of a role.
        To write this mathematically, we first define the variance ratio (in the space of measurements):
        $$ \varianceRatio = 
        \frac{\measurementMatrix \stateCovariance \measurementMatrix^T}{\measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance} =
        \frac{\text{Uncertainty from state estimate}}{\text{Total uncertainty from the state estimate and sensor noise}} $$
    </p>
    <p>TODO: Popout box:Hold on! I didn't realize you could write a matrix inverse like a fraction. How does that work? 
        Technically, writing a matrix fraction is not legitimate because matrices (unlike scalars) 
        very much care whether the numerator is multiplied on the left or right of the denominator, 
        but the fraction form does not contain this information. That being said, the insight that 
        this is a simple ratio is hieghtened by the fractional notation, so I ask that you simply 
        remember that the inverse goes on the right. 
    </p>

    <p>Then we can use this to write a weighted sum of each distribution:
        $$ \state_{\discreteTime} = \measurementMatrix^{-1} \left[ \left( \identityMatrix - \varianceRatio \right) \measurementMatrix \state + \varianceRatio \measurement \right] $$
    </p>

    <p>TODO: Popout box: Is there a deeper mathematical reason why we use the variance ratio? 
        Yes! You may have learned about Bayes' Rule, which describes how to update your beliefs 
        in light of new information. In particular, it says that 

        $$ P( A | B ) = \frac{P( B | A ) P( A )}{P( B )}$$
        $$ \text{Posterior} = \frac{\text{Likelihood} \times \text{Prior}}{\text{Normalizing constant}} $$

        In our case, the prior is the state estimate from the prediction step, 
        the likelihood comes from our measurement, and the posterior is our combined estimate.
        $$ P(\state | \measurement) = \frac{P(\measurement | \state) P(\state)}{P(\measurement)} $$

        The primary peculiarity of the Kalman Filter is that instead of having individual numbers 
        for each probability in the equation, now we have gaussian distributions. Nonetheless, the 
        principle of multiplying the likelihood by the prior and normalizing is the same. If you 
        multiply two gaussian probability density functions, you can actually show that
        the variance ratio emerges! For example, in 1D you could algebraically show :

        $$ C_1 \exp \left( -\frac{(x-\mu_1)^2}{2\sigma_1^2} \right) \times 
           C_2 \exp \left( -\frac{(x-\mu_2)^2}{2\sigma_2^2} \right) = 
           C_3 \exp \left( -\frac{(x-\mu_3)^2}{2\sigma_3^2} \right) $$

        Where the \( C \) terms are normalization constants, and the mean and variance of the resulting distribution are:
        $$ \mu_3 = \left( 1 - \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} \right) \mu_1 + 
                              \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} \mu_2 $$
        $$ \sigma_3^2 = \sigma_1^2 - \frac{\sigma_1^2}{\sigma_1^2 + \sigma_2^2} \sigma_1^2 $$
    </p>

    <p>
        Hold on, where did that \(\measurementMatrix^{-1}\) come from? Remember, at the end of the equation we need to get something that is in the space of states, 
        but we're currently in the space of measurements. Since \(\measurementMatrix\) converts from state space to measurement space, 
        we'd imagine that \(\measurementMatrix^{-1}\) goes the other way from measurements to states. 
    </p>
    <p> 
        There's only one problem: \(\measurementMatrix\) is not necessarily square, so it cannot be inverted. 
    </p>

    <p>
        Fortunately, there's a workaround! All we have to do is suspend our disbelief and pretend that 
        \(\identityMatrix = \measurementMatrix^{-1} \measurementMatrix\). Then we can distribute the \(\measurementMatrix^{-1}\) 
        into the formula and cancel it with existing \(\measurementMatrix\) terms, including the leading \(\measurementMatrix\) of 
        the variance ratio formula. The variance ratio without its leading \(\measurementMatrix\) is actually so important that 
        we'll give it a name:
        $$ \text{The Kalman Gain } \kalmanGain = \stateCovariance \measurementMatrix^T \left( \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right)^{-1} $$
        Then we can write the variance ratio as:
        $$ \varianceRatio = \measurementMatrix \kalmanGain $$
        Now we can proceed in applying the distributive property to get our state update equation:
        $$ \begin{align*}
         \state_{\discreteTime} &= \measurementMatrix^{-1} \left[ (\identityMatrix - \varianceRatio) \measurementMatrix \state_{\discreteTime-1} + \varianceRatio \measurement \right] \\
         &= \left[ \left( \measurementMatrix^{-1} - \measurementMatrix^{-1} \varianceRatio \right) \measurementMatrix \state_{\discreteTime-1} + \measurementMatrix^{-1} \varianceRatio \measurement \right] \\
         &= \left( \measurementMatrix^{-1} - \kalmanGain \right) \measurementMatrix \state_{\discreteTime-1} + \kalmanGain \measurement \\
         \state_{\discreteTime} &= \left( \identityMatrix - \kalmanGain \measurementMatrix \right) \state_{\discreteTime-1} + \kalmanGain \measurement \\
         \end{align*}
         $$
    </p>

    <p> Great! Now we have a matrix formula to combine the means of two Gaussians. The only step left is to propagate the variances too. 
        Remember our rule of thumb: For a gaussian \( \gaussian \left( \mathbf{\mu}, \mathbf{\Sigma} \right) \), 
        any time we apply a linear operation on the mean (eg. \( \mathbf{A} \mu \) ), 
        the variance will get that same operation applied twice (eg. \( \mathbf{A} \mathbf{\Sigma} \mathbf{A}^T \) ).
    </p>

    <p>
        Using this rule, we can easily write aout the variance:
        $$ \stateCovariance_{\discreteTime} = \left( \identityMatrix - \kalmanGain \measurementMatrix \right) \stateCovariance_{\discreteTime-1} \left( \identityMatrix - \kalmanGain \measurementMatrix \right)^T + \kalmanGain \measurementNoiseCovariance \kalmanGain^T $$
        And with a bit more algebra (TODO: expand this out) we can reduce this to:
        $$ \stateCovariance_{\discreteTime} = \stateCovariance_{\discreteTime-1} - \kalmanGain \measurementMatrix \stateCovariance_{\discreteTime-1} $$
    </p>

    <p>Option 2: See what states could have led to the measurement we got, and fuse this with our current state estimate.
    </p>
    <p> In order to put our two distributions on the same footing, we'll multiply the
        measurement distribution by \( \measurementMatrix^{-1} \) to bring it into 
        the space of states. Remembering our rule of thumb, when we multiply the 
        mean by \( \measurementMatrix^{-1} \),
        we also have to multiply the variance by \( \measurementMatrix^{-1} \) on the left and right.
        This will leave us with two normal distributions in the same space:
        <ul>
            <li>The state you Predicted: \( \gaussian \left( \state, \space \stateCovariance \right) \)</li>
            <li>The state implied by the measurement: \( \gaussian \left( \measurementMatrix^{-1} \measurement, \space \space \measurementMatrix^{-1} \measurementNoiseCovariance (\measurementMatrix^{-1})^T \right) \)</li>
        </ul>

        (If you see something wrong here, suspend your disbelief and read on ... )
    </p>
    <p>Now that we have two distributions in the same space, we can stop to think: what would we like to 
        be true when we combine these estimates? Here are some things that would be nice:
    </p>
    <ol>
        <li>The mean of the combined estimate should always lie between the means of the two original estimates</li>
        <li>The uncertainty (ie. variance) should be smaller than the uncertainty of either of the original estimates</li>
        <li>The resulting estimate should more closely resemble the distribution in which we have higher confidence (ie. less variance).</li>
    </ol>
    
    <p>For example, if we know that the ultrasonic sensor is much more accurate than our dynamics model (which is propagated blindly), then 
        we'd want our combined estimate to consist mostly of the sensor measurement, with a hint of the dynamics model 
        to account for small measurement noise. Since we're making use of all of our information, we should 
        be more certain of our combined estimate than either of the original estimates independently.
    </p>
    <p>But how much, exactly, of each estimate do we want to use? One reasonable approach is to weight each distribution 
        by the inverse of its variance, so the distribution with less uncertainty (ie. smaller variance) will play more of a role.
        To write this mathematically, we first define the variance ratio (in the space of states):
        $$ \varianceRatioState = \frac{\stateCovariance}{\stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance (\measurementMatrix^{-1})^T} $$
    </p>

    <p>Then we can use this to write a weighted sum of each distribution:</p>
    <p>
        $$ \state = (\identityMatrix - \varianceRatioState) \state + \varianceRatioState \measurementMatrix^{-1} \measurement $$
    </p>

    <p>
        Great! It seems like this weighted sum does exactly what we need.  
    </p>
    <p> 
        There's only one problem: \(\measurementMatrix\) is not necessarily square, so it cannot be inverted. 
    </p>
    <p>
        TODO: Make this popout box
        <img src="./assets/Grus-plan-Measurement-Matrix-Not-Invertible.png" alt="Gru's Plan meme: Measurement Matrix is not invertible" style="max-width: 100%; height: auto;">
    </p>

    <p> 
        That kind of throws a wrench in everything, even from the beginning. Fortunately, there is a workaround! 
        If we suspend our disbelief and do a bit of matrix algebra as if \( \measurementMatrix^{-1} \) exists,
        we can avoid ever having to invert the measurement matrix. We'll use the following identities:
        For any invertible matrices \(\mathbf{A}\), \(\mathbf{B}\) we have:
        \( (\mathbf{A} \mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1} \), and
        \( (\mathbf{A}^{-1})^{-T} = (\mathbf{A}^T)^{-1} \), 
        and we can always multiply anywhere by the identity \( \identityMatrix = \measurementMatrix^{-1} \measurementMatrix = \measurementMatrix^T (\measurementMatrix^T)^{-1} \)
    </p>

    <p>
        So let's knock out those inverses!
        $$ \begin{align*}
        \varianceRatioState &= \stateCovariance \left[ \stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right]^{-1} \\
        &= \stateCovariance \left[ \stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right]^{-1} \left( \measurementMatrix^{-1} \measurementMatrix \right)\\
        &= \stateCovariance \left[ \measurementMatrix \left( \stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right) \right]^{-1} \measurementMatrix \\
        &= \stateCovariance \left[ \measurementMatrix \stateCovariance + \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right]^{-1} \measurementMatrix \\
        &= \stateCovariance \left( \measurementMatrix^T \left( \measurementMatrix^T \right)^{-1} \right) \left[ \measurementMatrix \stateCovariance + \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right]^{-1} \measurementMatrix \\
        &= \stateCovariance \measurementMatrix^T \left[ \left( \measurementMatrix \stateCovariance + \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right) \measurementMatrix^T \right]^{-1} \measurementMatrix \\
        \varianceRatioState &= \stateCovariance \measurementMatrix^T \left[ \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right]^{-1} \measurementMatrix \\
        \end{align*}
        $$
    </p>
    <p> Now our Variance ratio is free of \( \measurementMatrix^{-1} \), but there is still one 
        more remaining in the weighted sum formula. In preparation to eliminate this remaining 
        \( \measurementMatrix^{-1} \), we'll separate out the trailing \( \measurementMatrix \) 
        from the rest of the formula. The variance ratio without the trailing \( \measurementMatrix \) 
        is actually so important that we'll give it a name:
        $$ \text{The Kalman Gain } \kalmanGain = \stateCovariance \measurementMatrix^T \left[ \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right]^{-1} $$
        Then we can write the variance ratio as:
        $$ \varianceRatioState = \kalmanGain \measurementMatrix $$
    </p>

    <p>
        Now we can proceed in writing our state update equation without any remaining \( \measurementMatrix^{-1} \):
        $$ \begin{align*}
        \state &= (\identityMatrix - \varianceRatioState) \state + \varianceRatioState \measurementMatrix^{-1} \measurement \\
        \state &= \left( \identityMatrix - \kalmanGain \measurementMatrix \right) \state + \kalmanGain \measurement \\
        \end{align*}
        $$
    </p>

    <p> Great! Now we have a matrix formula to combine the means of two Gaussians. The only step left is to propagate the variances too. 
        Remember our rule of thumb: For a gaussian \( \gaussian \left( \mathbf{\mu}, \mathbf{\Sigma} \right) \), 
        any time we apply a linear operation on the mean (eg. \( \mathbf{A} \mu \) ), 
        the variance will get that same operation applied twice (eg. \( \mathbf{A} \mathbf{\Sigma} \mathbf{A}^T \) ).
    </p>

    <p>
        Using this rule, we can easily write aout the variance:
        $$ \stateCovariance_{\discreteTime} = \left( \identityMatrix - \kalmanGain \measurementMatrix \right) \stateCovariance_{\discreteTime-1} \left( \identityMatrix - \kalmanGain \measurementMatrix \right)^T + \kalmanGain \measurementNoiseCovariance \kalmanGain^T $$
        And with a bit more algebra (TODO: expand this out) we can reduce this to:
        $$ \stateCovariance_{\discreteTime} = \stateCovariance_{\discreteTime-1} - \kalmanGain \measurementMatrix \stateCovariance_{\discreteTime-1} $$
    </p>

    <h2>Bringing it all together</h2>
    <p>
        In each of our derivations, we've arrived at the same definition for the Kalman Gain: 
        $$ \kalmanGain = \stateCovariance \measurementMatrix^T \left( \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right)^{-1} $$

        However, we have also shown that there are two different ways to interpret this formula (so 
        long as you imagine that \( \measurementMatrix^{-1} \) exists). Rearranging formulas X and Y (TODO)
        we have:
        $$ \kalmanGain = \measurementMatrix^{-1} \varianceRatio = \varianceRatioState \measurementMatrix^{-1} $$

        Therefore, we can choose to interpret the Kalman Gain as a two-step process that always takes measurements as inputs, then either:
        <ul>
            <li>weighs the input by the measurement-space variance ratio, then converts to state space</li>
            <li>converts the input to state space, then weights it by the state-space variance ratio</li>
        </ul>
    </p>

    <h2>The Full Algorithm</h2>
    <h3>Eg. What you actually need to implement</h3>
    Now we have all the pieces to write out the full algorithm, but we'll make a few adjustments to make it more numberically stable
    <ul>
        <li>(I-KH) can be numerically unstable, so we distribute it out to get</li>
        <li>All of the matrices (F,B,H,etc.) can vary with time, so we'll subscript them with the current timestep</li>
        <li>You should be calling the Predict function as often as you can (the dynamics never stops so neither should your blind estimate),
            but the update function can only be called when there is a new measurement. If your
            sensor is fast, you might always call Predict immediately followed by Update, 
            but if your sensor is slow, 5 predict steps might go by with no update. 
            I've accounted 
        </li>
    </ul>

    <table>
        <tr>
            <th>Predict(\( \controlInput_{\discreteTime}, \space \state_{\discreteTime-1}, \space \stateCovariance_{\discreteTime-1} \))</th>
            <th>Update(\( \measurement_{\discreteTime}, \space \state_{\discreteTime-1}, \space \stateCovariance_{\discreteTime-1} \))</th>
        </tr>
        <tr>
            <th>    
                $$ \state_{\discreteTime} = \stateTransition \state_{\discreteTime-1} + \controlMatrix \controlInput_{\discreteTime} $$
                $$ \stateCovariance_{\discreteTime} = \stateTransition \stateCovariance_{\discreteTime-1} \stateTransition^T + \processNoiseCovariance $$
            </th>
            <th>    
                $$ \kalmanGain = \stateCovariance_{\discreteTime-1} \measurementMatrix^T \left( \measurementMatrix \stateCovariance_{\discreteTime-1} \measurementMatrix^T + \measurementNoiseCovariance \right)^{-1} $$
                $$ \state_{\discreteTime} = \state_{\discreteTime-1} + \kalmanGain \left( \measurement_{\discreteTime} - \measurementMatrix \state_{\discreteTime-1} \right) $$
                $$ \stateCovariance_{\discreteTime} = \stateCovariance_{\discreteTime-1} - \kalmanGain \measurementMatrix \stateCovariance_{\discreteTime-1} $$
            </th>
        </tr>
    </table>

    <h2>Summary</h2>
    <p>
        The <strong>Kalman Filter</strong> is an algorithmic blueprint for incorporating different sources 
        of information into a unified probailistic state estimate. It outlines two ways to 
        propagate the current state estimate to the next:
        <ol>
            <li>Predict: Incorporate information about the default dynamics of the system</li>
            <li>Update: Fuse sensor information with the most recent state estimate 
                using the Kalman Gain</li>
        </ol>
        The <strong>Kalman Gain</strong> is an operator that takes vectors in the 
        measurement space as input, and outputs the weighted contribution of that measurement 
        vector to the total state estimate. This operation can be interpreted as having two 
        steps (in either order):
        <ul>
            <li>Convert from the space of measurements to the space of states</li>
            <li>Weight the contribution of the measurement to the state estimate in proportion
                to the inverse of the measurement's variance</li>
        </ul>
        Note that in this interpretation, the inverse measurement matrix <span id="measurement-matrix-inverse"></span>
        is imagined to convert back from the space of measurements to the space of states, even though
        in practice it is <em>almost never invertible</em>. This is why the Kalman Filter is usually 
        defined without it. 
    </p>
    <p>
        The Kalman Gain has been proven to be the Best Linear Unbiased Estimator (BLUE) for 
        gaussian random variables.
    </p>
    <p>Hopefully while playing you realized that 1) it's not magic, and 2) there are many obvious shortcomings. For example, 
        What if your model isn't linear? What if you don't know the covariance matrix for your sensors? How do small changes 
        in parameters like the timestep affect the filter's behavior? The key takeaway is the underlying principles: 
        how to combine and propagate gaussian random variables.
    </p>

    <h2>Model Summary</h2>
    I've just thrown you 15 variables -- that can be a lot to keep track of! I've tabulated all of them here,
    but throughout the article you can also click on any variable to see its name, and keep clicking to get more information.
    You can also choose the naming convention using the dropdown menu above, which should make it easier to learn across multiple sources.

    Link to Visually Explained:https://www.youtube.com/watch?v=IFeCIbljreY
    Link to Steve Brunton: https://www.youtube.com/watch?v=s_9InuQAx-g

    TODO: Make the highilght colors match the diagram colors

    What made me care?
        Powerful way of combining information in the face of uncertainty
        Elegant Combination of linear algebra and probability theory,
        Lightweight tool used for pretty much any system that are continuously changing
        Used everywhere there is time-series data: Target tracking, navigation, control, robotics, neuroscience, finance
        
    <h2>Appendix</h2>
    Many texts make the distinction between the estimate of the state and the <em>true</em> state. Why don't I?
    I did this because when I was learning, I found that with some effort I could distinguish between 
    \( \state \) and \( \hat{\state} \) and \( \bar{\state} \), as well as terms like "a priori" 
    ( \(x_{\discreteTime | \discreteTime-1} \)) and "a posteriori" ( \(x_{\discreteTime | \discreteTime} \)).
    but ultimately those distinctions did not speed my learning process or aid my intuition.

    If you distinguish between estimated state and true state, then you have to be able to think in 3 layers:
    true reality, your model of reality, and then your algorithm, which tracks your model of reality. 

    From the perspective of someone who wants to gain an intuition for how the filter works and possibly implement one 
    (but not necessarily rigorously prove its optimality in some theoretical environment), the distinction
    between model and algorithm is just extra fluff. In practice there are only two processes happening: reality, 
    and the algorithm that tracks reality. When learning and implementing, it is easier to treat the algorithm as 
    both theoretical model and algorithm, and simplify the notation. We admit that our theoretical model of 
    reality is wrong (eg. the assumption of linearity and gaussian noise), but we can simply add these to the 
    list of things that make our algorithm differ from reality (like choosing a timestep and noise parameters).

    I will be the first to admit that I know relatively little about rigorously proving the optimality of the Kalman Filter
    in a theoretical context (despite having sat through lectures that proved it). I think it may be possible to 
    rigorously unite the algorithm with the model (and by that I mean stop distinguishing between estimate and true state), 
    but, not having done so, I cannot say that that distinction is useless in general.    
</body>
</html>