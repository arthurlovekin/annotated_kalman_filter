<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Interactive Kalman Filter</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>

    <script src="js/draggable-number.js"></script>
    <script src="js/gaussian-functions.js"></script>
    <script src="js/formula-elements.js"></script>
    <script src="js/mouse-effects-manager.js"></script>
    <script>
        const mouseEffectsManager = new MouseEffectsManager([...Object.values(variables)]);
    </script>
    <script>
        MathJax = {
            //Enable macros \class, \cssId, \href, \style
            loader: {load: ['[tex]/html']}, 
            tex: {packages: {'[+]': ['html']}},
            // save custom TexMacros in the MathJax configuration object 
            // (becomes MathJax.config.TexMacros after MathJax is loaded)
            TexMacros: {
                // Note: use two '\'s because this is first intepreted as a javascript strings, where '\' needs to be escaped
                wikipedia: `
                \\def\\state{\\class{state}{\\mathbf{\\bar{x}}}}
                \\def\\stateEstimate{\\class{state-estimate}{\\mathbf{x}}}
                \\def\\stateTransition{\\class{state-transition}{\\mathbf{F}}}
                \\def\\controlInput{\\class{control-input}{\\mathbf{u}}}
                \\def\\measurement{\\class{measurement}{\\mathbf{z}}}
                \\def\\measurementNoiseCovariance{\\class{measurement-noise-covariance}{\\mathbf{R}}}
                \\def\\measurementMatrix{\\class{measurement-matrix}{\\mathbf{H}}}
                \\def\\stateCovariance{\\class{state-covariance}{\\mathbf{P}}}
                \\def\\processNoiseCovariance{\\class{process-noise-covariance}{\\mathbf{Q}}}
                \\def\\controlMatrix{\\class{control-matrix}{\\mathbf{B}}}
                \\def\\kalmanGain{\\class{kalman-gain}{\\mathbf{K}}}
                \\def\\varianceRatio{\\class{variance-ratio}{\\widetilde{\\mathbf{K}}_{z}}}
                \\def\\varianceRatioState{\\class{variance-ratio-state}{\\widetilde{\\mathbf{K}}_{x}}}
                \\def\\discreteTime{n}
                \\def\\identityMatrix{\\class{identity-matrix}{\\mathbf{I}}}
                \\def\\gaussian{\\class{gaussian-distribution}{\\mathcal{N}}}
                `,
                kalmanfilterdotnet: `
                \\def\\state{\\class{state}{\\mathbf{\\bar{s}}}}
                \\def\\stateEstimate{\\class{state-estimate}{\\mathbf{s}}}
                \\def\\stateTransition{\\class{state-transition}{\\mathbf{A}}}
                \\def\\controlInput{\\class{control-input}{\\mathbf{u}}}
                \\def\\measurement{\\class{measurement}{\\mathbf{z}}}
                \\def\\measurementNoiseCovariance{\\class{measurement-noise-covariance}{\\mathbf{R}}}
                \\def\\measurementMatrix{\\class{measurement-matrix}{\\mathbf{C}}}
                \\def\\stateCovariance{\\class{state-covariance}{\\mathbf{P}}}
                \\def\\processNoiseCovariance{\\class{process-noise-covariance}{\\mathbf{Q}}}
                \\def\\controlMatrix{\\class{control-matrix}{\\mathbf{B}}}
                \\def\\kalmanGain{\\class{kalman-gain}{\\mathbf{K}}}
                \\def\\varianceRatio{\\class{variance-ratio}{\\widetilde{\\mathbf{K}}_{z}}}
                \\def\\varianceRatioState{\\class{variance-ratio-state}{\\widetilde{\\mathbf{K}}_{x}}}
                \\def\\discreteTime{t}
                \\def\\identityMatrix{\\class{identity-matrix}{\\mathbf{I}}}
                \\def\\gaussian{\\class{gaussian-distribution}{\\mathcal{N}}}
                `
            },
          // Custom function to get the desired set of macros,
          // then have the renderer to go back to the compile step so the new macros get used.
          setMacros(name) {
            if (!MathJax.config.TexMacros.hasOwnProperty(name)) {
              console.warn(`TexMacros for "${name}" not found.`);
              return Promise.resolve();
            }
            
            return MathJax.startup.promise = MathJax.startup.promise.then(() => {
              const {mathjax} = MathJax._.mathjax;
              const {STATE} = MathJax._.core.MathItem;
              MathJax.tex2mml(MathJax.config.TexMacros[name]);
              mathjax.handleRetriesFor(() => MathJax.startup.document.rerender(STATE.COMPILED));
              
              mouseEffectsManager.initialize();
            });
          },
          startup: {
            typeset: false, //disable the initial typesetting pass since setMacros() will do typesetting
            ready() {
              MathJax.startup.defaultReady();
              MathJax.config.setMacros('wikipedia');
              const dropdown = document.getElementById('naming-convention-dropdown');
              if (!dropdown) {
                console.warn('MathJax ready(): No dropdown element found. Not changing from the default.');
                return;
              }
              dropdown.addEventListener('input', (event) => MathJax.config.setMacros(event.target.value));
            }
          }
        }
    </script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        /* Formatting for the entire page */
        body {
            font-family: serif;
            margin: 40px auto;
            /* padding: 40px; This made the text adjust too late */
            background-color: hsl(40, 100%, 98%);
            color: hsl(208, 30%, 18%);
            max-width: 45rem; /* Adjust this value as needed for desired width */
            width: 100%;
        }

        h1, h2 {
            font-family: sans-serif;
        }

        .matrix-table {
            position: relative;
        }
        /* Rectangular pseudo-element before and after the table */
        .matrix-table::before,
        .matrix-table::after {
            content: "";
            position: absolute;
            top: 0;
            bottom: 0;
            width: 4px; 
        }
        /* Make the left pseudo-element look like a bracket */
        .matrix-table::before {
            left: 0px;
            border-left: 2px solid black;
            border-top: 2px solid black;
            border-bottom: 2px solid black;
        }
        /* Make the right pseudo-element look like a bracket */
        .matrix-table::after {
            right: 0px;
            border-right: 2px solid black;
            border-top: 2px solid black;
            border-bottom: 2px solid black;
        }
        .matrix-table td {
            text-align: center;
            vertical-align: middle;
            width: 6ch; /* matches the DraggableNumber width */
        }
        .matrix-table tr {
            line-height: 1.5;
        }

        .math-diagram-structure-table {
            border-collapse: collapse;
            margin-left: auto;
            margin-right: auto;
        }

        .math-diagram-structure-table td {
            text-align: center;
            vertical-align: middle;
        }

        .kalman-gain-state-space {
            background-color: rgb(187, 187, 187);
        }

        .kalman-gain-intermediate-space {
            background-color: #dddddd;
        }

        .kalman-gain-measurement-space {
            background-color: rgb(255, 252, 238);
        }

        #kalman-gain-Hstate-gaussian {
            border: 2px solid rgb(0, 0, 255);
            border-radius: 10px;
        }

        #kalman-gain-measurement-gaussian {
            border: 2px solid rgb(255, 0, 0);
            border-radius: 10px;
        }

        #kalman-gain-combined-gaussian {
            border: 2px solid rgb(0, 150, 0);
            border-radius: 10px;
        }

        /* Outline the table temporarily so I can see what I'm doing */

        /* .math-diagram-structure-table {
            border: 1px solid #ddd;
        } */

        /* .math-diagram-structure-table td {
            border: 1px solid #ddd;
        }

        .math-diagram-structure-table tr:nth-child(even) {
            background-color: #f2f2f2;
        } */

    </style>
</head>

<body>
    <select id="naming-convention-dropdown">
        <option value="wikipedia">Wikipedia</option>
        <option value="kalmanfilterdotnet">KalmanFilter.net</option>
    </select>

    <h1>The Interactive Kalman Filter</h1>

    <h2>Introduction</h2>
    <p>The Kalman Filter is a powerful tool for combining information in the face of uncertainty,
        and is used widely throughout science and engineering for tracking dynamic systems. 
        Unfortunately, most of the explanations you'll find are either mathematically dense to 
        the point of being impenetrable, and/or leave a lot of questions remaining as to why we 
        intuitively should expect the equations to look as they do. In this article, I hope to 
        teach you how deep principles in probability and linear algebra (which you probably 
        already know!) naturally lead to the equations of the Kalman Filter. I've also made 
        a series of interactive diagrams that will hopefully give you a better 
        feel for the different parameters in all of their intricacy, and understand both the power and limitations
        of this method.
    </p>

    <h3>What is the Kalman Filter?</h3>
    <p>
    The primary goal of the Kalman Filter is to estimate the state of a dynamical system.
    The need for state estimation shows up throughout engineering: if you're trying to drive a car, you first need to know where you
    are and how fast you're going before you can intelligently turn the steering wheel.
    If you're controlling the central heating in a building, it's very helpful to know
    the temperature in the rooms. The challenge is that although we often have many sensors,
    they are usually noisy and cannot individually perceive the full state. These sensors need 
    to be combined with each other and with the engineer's knowledge of the system in order 
    to get a complete picture.

    The Kalman Filter is a theoretically optimal way to combine different sources of information into one unified state estimate. 

    </p>

    <h2>Building a model</h2>
    <p>The easiest way to wrap your head around the Kalman Filter is with an example.
        Throughout this artile, we'll use this simple robot.
    </p>
    <p>We begin with a large question: How to model the world? A common engineering approach is 
        to think of the minimum set of variables needed to make a complete description of the system, 
        and then put them all together in a state vector. In our simple example, all we need is the position \(p\) 
        and velocity \(v\) of the robot. 
    </p>
    <p>TODO: put in popout box:     In general, the state vector could include many different quantities: in
        Fluid dynamics it would be things like temperature, pressure, flow rate,
        volume, and in a population dynamics model it might include the number of
        individuals and their age distribution.
    </p>
    <p>$$ \text{State vector }\stateEstimate = \begin{bmatrix} p \\ v \end{bmatrix} 
            = \begin{bmatrix} \text{position} \\ \text{velocity} \end{bmatrix} $$
    </p>
    <p> Once we've defined our state vector, we need to model how it evolves over time.
    For our robot, we can use a simple kinematics formula:
    $$ \begin{alignat}{3}
        p_{\discreteTime} &= p_{\discreteTime-1} &+& \Delta t &v_{\discreteTime-1} \\
        v_{\discreteTime} &= 0                   &+&          &v_{\discreteTime-1} \\
        \end{alignat}
    $$
    </p>
    <p>We'll package this up into matrix form so it is easy to use later.
        We'll call the matrix that converts from the old state to a new state 
        using the dynamics model the state transition matrix \(\stateTransition\).

        $$ \text{State Transition Matrix } \stateTransition = \begin{bmatrix} 1 & \Delta t \\ 0 & 1 \end{bmatrix} $$
        $$
        \begin{alignat}{3}
            \begin{bmatrix}
                p_{\discreteTime} \\
                v_{\discreteTime}
            \end{bmatrix}
            &=
            \begin{bmatrix}
                1 & \Delta t \\
                0 & 1
            \end{bmatrix}
            &
            \begin{bmatrix} 
                p_{\discreteTime-1} \\ 
                v_{\discreteTime-1} 
            \end{bmatrix} \\

            \stateEstimate_{\discreteTime} &= \stateTransition & \stateEstimate_{\discreteTime-1} \\
        \end{alignat}
        $$
    </p>

    <p>
        Now we have a simple dynamical model of our robot, but it fails to include one
        thing that is common to most robots: a way to control it. In the demo above, you
        (as the operator) are able to control the robot with a throttle command \(u\),
        which can range between [0,1]. Inherently this [0,1] value has no meaning, but
        the robot is programmed so that the throttle controls the angular speed of
        the wheels, which in turn determines the robot's linear velocity.
        If we assume that the has radius \(r\), the maximum angular speed
        is \(\omega_{max} \) (and that the wheel speed perfectly matches the control 
        command), then we have the following relationship between the throttle command \(u\) and the velocity \(v\) of the robot:

        $$ v = r \omega = r \omega_{max} u $$

        As with the state equation, we'll package this up in matrix form by defining
        the control matrix \(\controlMatrix\), which determines how the input 
        vector \(\controlInput \) impacts the state. Note that the throttle
        has no effect on the position, so the first row of \(\controlMatrix\) is zero.

        $$ \text{Control Input Vector } \controlInput = \begin{bmatrix} u \end{bmatrix} $$

        $$ \text{Control Matrix } \controlMatrix = \begin{bmatrix} 0 \\ r \omega_{max} \end{bmatrix} $$
        
        Combining this with our dynamics model, our complete dynamical model looks like this:

        $$
        \begin{alignat}{5}
            \begin{bmatrix}
                p_{\discreteTime} \\
                v_{\discreteTime}
            \end{bmatrix}
            &=
            \begin{bmatrix}
                1 & \Delta t \\
                0 & 1
            \end{bmatrix}
            &
            \begin{bmatrix} 
                p_{\discreteTime-1} \\ 
                v_{\discreteTime-1} 
            \end{bmatrix}
            &+
            \begin{bmatrix}
                0 \\
                r \omega_{max} \\
            \end{bmatrix}
            &
            \begin{bmatrix}
                u_{\discreteTime} \\
            \end{bmatrix} \\
            \stateEstimate_{\discreteTime} &= \stateTransition & \stateEstimate_{\discreteTime-1} &+ \controlMatrix & \controlInput_{\discreteTime} \\
        \end{alignat}
        $$
    </p>
    <h2>Bringing Uncertainty into the Model</h2>
    <p>
        The linear model we derived above is great and all, but let's be honest, it's not very realistic.
        In the real world, the robot might slip and bounce over the terrain, the motors might not
        exactly reach the commanded speed. In other words, there is random noise introduced by the 
        messiness of the environment, which in turn makes our state estimate less certain.
    </p>
    <p>
        One method that can represent both the uncertainty in our state estimate 
        and the random noise introduced by the environment is to assign each of them a 
        probability distribution. For simplicity, we assume that both of these distributions are gaussian.
    </p>

    <p> TODO: Put this in a popout box
        Why a gaussian? There are a few really compelling reasons: 
        <ol>
            <li>We can parametrize the distribution with just the mean and variance, so it is numerically easy to update</li>
            <li>When we combine two independent gaussians into one estimate, the result is also a gaussian</li>
            <li>Mathematicians have proven (the Central Limit Theorem)that the combined effect of many random processes (even if individually they are non-gaussian)
                will result in a gaussian distribution</li>
        </ol>

        Take heed: we choose a gaussian mostly because it is numerically convenient, but in practice many things (even something as simple
        as a \( sin \theta \) term in the dynamics model introduced by turning around), will make the distribution non-gaussian.
    </p>
    <p> So now, instead of representing our state estimate with a single vector, we'll say that our state estimate 
        is a normal distribution parametrized by a mean \(\state\) and a covariance \(\stateCovariance\).
        $$ \stateEstimate \sim \gaussian \left( \state, \stateCovariance \right) $$
    </p>
    <p>
        Hopefully you've already seen a 1D normal distribution
    </p>
    
    <p> TODO: Diagram: Play around with a single gaussian: Covariance, mean</p>

    <p>Now comes an important point: In our dynamics equation above we know how to apply the matrix \(\stateTransition\) 
        to the state estimate \( \stateEstimate \) when it was a single vector. But now our estimate is a gaussian,
        so we need to apply \(\stateTransition\) to the whole distribution. Fortunately, the mean is a single point,
        so we can directly apply the state transition matrix as we did before. The variance on the other hand is the 
        average "squared-distance" from the mean, so how should it be treated?
    </p>
    <p> We'll introduce a very useful rule:
        In order to multiply a gaussian distribution \(\gaussian \left( \mathbf{\bar{\mu}}, \mathbf{\Sigma} \right) \) by a matrix \( \mathbf{A} \), 
        the mean is simply multiplied normally, while the covariance is multiplied by \( \mathbf{A} \) squared.
    </p>
    <p>
        Linear Transformation of a Normal Distribution:
        $$
        \mathbf{\bar{\mu}}_{\discreteTime} = \mathbf{A} \mathbf{\bar{\mu}}_{\discreteTime-1} 
        $$
        $$
        \mathbf{\Sigma}_{\discreteTime} = \mathbf{A} \mathbf{\Sigma}_{\discreteTime-1} \mathbf{A}^T
        $$
    </p>
    <p>
        One way you can remember this is to think back to a 1D normal distribution. The mean is a single number \(\mu\)
        and the variance is the standard deviation \(\sigma\) squared. When we multiply by a scalar \(a\), the mean 
        simply scales by \(a\), and the standard deviation also scales by \(a\) (because it is a distance on the plot).
    </p>

    <p>With this probabilistic knowledge, we have actually arrived to the point where we can write out the first component of the Kalman Filter:
        the <strong>Predict Step</strong>! We'll define the covariance of the state estimate to be \(\stateCovariance\), and the mean to be \(\state\). 
        To handle environmental noise, we'll also add a noise term with mean 0 and covariance \(\processNoiseCovariance\). Writing this out, we have:
    </p>
    <p>The Predict Step:
        $$ \state_{\discreteTime} = \stateTransition \state_{\discreteTime-1} + \controlMatrix \controlInput_{\discreteTime} $$
        $$ \stateCovariance_{\discreteTime} = \stateTransition \stateCovariance_{\discreteTime-1} \stateTransition^T + \processNoiseCovariance $$
    where
    $$ \stateEstimate \sim \gaussian \left( \state, \stateCovariance \right) $$
    and $$ \text{process noise} \sim \gaussian \left( 0, \processNoiseCovariance \right) $$
    </p>
    <p> 
        Fundamentally, the predict step describes how to <it>predict</it> the a new gaussian state estimate from the previous estimate, 
        based on our dynamical model of the system. Let's try running this algorithm in state space: 
    </p>

    <p> TODO: Diagram: Predict Step State Space diagram</p>
    
    <h2>Sensing the environment</h2>
    <p>Ok so the state ... isn't terrible, but it's not great either, and the variance is exploding (TODO: Popout box explaining why this makes sense)..
        Fortunately, we have endowed this robot with an ultrasonic distance sensor that should help.         
        The only question is, how do we incorporate these measurements into the filter? 
    </p>
    <p>
        To start with, we need a linear model that relates the measurement vector \( \measurement \) to the state \( \trueState \).
        Our first instinct might be to write a matrix that converts from the measurement vector to the state vector 
        (we're trying to update the state after all). The problem is 
        that since the sensor doesn't measure the velocity, there is no way to fully recover the state from the measurement 
        using a linear transformation. We can however think about this from a more causal perspective: 
    </p>
    <p>
        In reality we know that the robot is in some true state x, and when the sensor takes a measurement
        of that state it will generally follow a linear process. If we define the measurement matrix \(\measurementMatrix\)
        to describe this linear measurement process, then in the ideal case we can write:
        $$ \measurement_{ideal} = \measurementMatrix \trueState $$
    </p>
    <p>Let's get into some implementation details to see what \(\measurementMatrix\) looks like. 
        Most ultrasonic sensors do not directly provide a distance. Instead, they measure 
        the time (in microseconds, (\ \mu s \)) it takes for a pulse of sound to travel to the wall and back. In order to convert from (\ \mu s \)
        to meters, we need to multiply by a factor of \( \frac{2 \times 10^6 }{c} \), where \(c\) is the speed of sound in air (343 m/s), 
        there are \( 10^6 \mu s\) per second, and the factor of 2 accounts for the sound traveling double the distance (to the wall, and then back).
        Thus to convert from the measurement vector to the state vector, the measurement matrix would be:
        $$ \measurementMatrix = \begin{bmatrix} \frac{2 \times 10^6 }{c} & 0 \end{bmatrix} $$
    </p>
    <p>Of course, no sensor is ever perfect, so we will also say that when the sensor takes a measurement 
        it will also introduce some sensor noise. We assume that the noise is normally distributed with 
        zero mean and covariance \(\measurementNoiseCovariance\) (which hopefully is listed in the sensor datasheet).
        $$ \text{Measurement Noise} \sim \gaussian \left( 0, \measurementNoiseCovariance \right) $$

        Now instead of the ideal measurement model above, we will say that the measurements you'd expect
        to get from the true state follow a normal distribution with covariance \(\measurementNoiseCovariance\) and mean \(\measurementMatrix \trueState\).
        In practice however, \(\measurementMatrix \trueState\) is unobservable; we only ever get a single measurement, which is a 
        sample from this distribution. Since \( \measurement \) is the only sample we have, we approximate the 
        distribution of possible measurements from true state x as a gaussian centered at \( \measurement \) with covariance \(\measurementNoiseCovariance\):
        $$ \measurement \sim \gaussian \left( \measurementMatrix \trueState, \measurementNoiseCovariance \right) $$
    </p>
    
    <h2>Update Step: Combining two gaussians</h2>
    <p>With this measurement model in hand, we're ready to fuse it with our previous state estimate.
        At this point, we have one normal distribution that represents our belief of the true state of the robot,
        and another normal distribution that represents the measurements you'd expect to get from the true state. 
        How can we combine these two distributions into one new state estimate? 
    </p>
    <p>First of all, what we would like to be true of our combined estimate?</p>
    <ol>
        <li>The mean of the combined estimate should always lie between the means of the two original estimates</li>
        <li>The uncertainty (ie. variance) should be smaller than the uncertainty of either of the original estimates (we have more information)</li>
        <li>The resulting estimate should more closely resemble the distribution in which we have higher confidence (ie. less variance).</li>
    </ol>
    
    <p>For example, if we know that the ultrasonic sensor is much more accurate than our blind dynamics model, then 
    we'd want our combined estimate to consist mostly of the sensor measurement, but it should be adjusted slightly by the dynamics model.
    We would be more confident of our combined estimate than either of the original estimates independently.</p>

    <p>To capture the concept of "giving heavier weight to the estimate with less variance", one reasonable approach is 
    to define the variance ratio: 
    Then we can take a weighted sum of the means (a linear operation), where each mean is weighed by the inverse of the 
    variance ratio, and apply the corresponding linear operation to update the variances. 
    We'll weight the current current state by (1- state variance/total variance)  and </p>

    <p>The ratio of variances kind of makes sense, but why choose this as opposed to something else? 
        In general, to combine two probability distributions you use the operation of convolution. [links to why]
        If you convolve two gaussians in 1D you can algebraically see that ratio emerges.
    </p>

    <p>In other words, we want to do something like: 
        $$\state_{\discreteTime} = ( \identityMatrix-\widetilde{\kalmanGain}) \state + \widetilde{\kalmanGain} \measurement, \space \space \space \text{  where } \space \widetilde{\kalmanGain} = \frac{\text{state variance}}{\text{state variance} + \text{measurement variance}} $$
    </p>

    <p>But here we run into another challenge: not only are we applying a weight, but we are also trying to combine different "types" of things.
    A state estimate is a vector with a particular dimension and units,
    while the measurements coming from the sensors have entirely different dimensions and units. As it is written above, the operation above is not defined, 
    because on the one hand when K converts x to x+1 it simply applies the weight (always stays in state space), but when it multiplies by Z
    it is both applying the weight and converting to state space. Another way to see this would be to look at the dimensions: If Kx is valid then K must be (..., state_dim), but if Kz is valid then K must be (..., measurement_dim). In general, 
    this is a contradiction. K has to either be multiplied by something in state space, or something in measurement space, but can't do both.</p>

    <p>As it happens, there are two ways you could fix this problem:</p>

    <p>You could either</p>
    <ol>
        <li>Convert the state into measurement space using H, then define the variance ratio in measurement space:</li>
        <li>Convert the measurement into state space using H^{-1}, then define the variance ratio in state space:</li>
    </ol>

    <p> Option 1:
        <ul>
            <li>The measurement you'd expect at the state you Predicted: \( \gaussian \left( \measurementMatrix \state, \space \measurementMatrix \stateCovariance \measurementMatrix^T \right) \)</li>
            <li>The measurement you actually got: \( \gaussian \left( \measurement, \space \measurementNoiseCovariance \right) \)</li>
        </ul>
    </p>

    <p>
        The Variance Ratio, in measurement space:
        $$ \varianceRatio = \frac{\measurementMatrix \stateCovariance \measurementMatrix^T}{\measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance} $$
    </p>

    <p>Hold on! I didn't realize you could write a matrix inverse like a fraction. How does that work? 
        Technically, writing a matrix fraction is not legitimate because matrices (unlike scalars) 
        very much care whether they're multiplied on the left or the right. Putting matrices in a fraction is a bit problematic
        because you can't tell whether the denominator is supposed to be on the left or right of the numerator. 
        That being said, there is insight to be gained by treating it like a fraction, so as long as you remember that in reality the 
        inverse goes on the right, this is a useful way to think about it. 
    </p>
    <p> 
        Since \(\varianceRatio\) is in measurement space, we have to convert the state estimate into measurement space 
        using \(\measurementMatrix\) before taking the ratio. Then our weighted sum is:

        $$ \state_{\discreteTime} = \measurementMatrix^{-1} \left[ (\identityMatrix - \varianceRatio) \measurementMatrix \state_{\discreteTime-1} + \varianceRatio \measurement \right] $$
    
        Hold on, where did that \(\measurementMatrix^{-1}\) come from? Remember, at the end of the equation we need to get something that is in the space of states, 
        but we're currently in the space of measurements. Since \(\measurementMatrix\) converts from state space to measurement space, 
        we'd imagine that \(\measurementMatrix^{-1}\) goes the other way from measurements to states. 
    </p>

    <p> 
        There's only one problem: \(\measurementMatrix\) is not necessarily square, so it cannot be inverted. 
    </p>

    <p>
        Fortunately, there's a workaround! All we have to do is suspend our disbelief and pretend that 
        \(\identityMatrix = \measurementMatrix^{-1} \measurementMatrix\). Then we can distribute the \(\measurementMatrix^{-1}\) 
        into the formula and cancel it with existing \(\measurementMatrix\) terms, including the leading \(\measurementMatrix\) of 
        the variance ratio formula. The variance ratio without its leading \(\measurementMatrix\) is actually so important that 
        we'll give it a name:
        $$ \text{The Kalman Gain } \kalmanGain = \stateCovariance \measurementMatrix^T \left( \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right)^{-1} $$
        which can be intuitively understood as: 
        $$ \kalmanGain = \measurementMatrix^{-1} \varianceRatio = \measurementMatrix^{-1} \left( \frac{\measurementMatrix \stateCovariance \measurementMatrix^T}{\measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance} \right) $$
       Now we can proceed in applying the distributive property to get our state update equation:
        $$ \begin{align*}
         \state_{\discreteTime} &= \measurementMatrix^{-1} \left[ (\identityMatrix - \varianceRatio) \measurementMatrix \state_{\discreteTime-1} + \varianceRatio \measurement \right] \\
         &= \left[ \left( \measurementMatrix^{-1} - \measurementMatrix^{-1} \varianceRatio \right) \measurementMatrix \state_{\discreteTime-1} + \measurementMatrix^{-1} \varianceRatio \measurement \right] \\
         &= \left( \measurementMatrix^{-1} - \kalmanGain \right) \measurementMatrix \state_{\discreteTime-1} + \kalmanGain \measurement \\
         \state_{\discreteTime} &= \left( \identityMatrix - \kalmanGain \measurementMatrix \right) \state_{\discreteTime-1} + \kalmanGain \measurement \\
         \end{align*}
         $$

    </p>

    <p> Great! Now we have a matrix formula to combine the means of two Gaussians. The only step left is to propagate the variances too. 
        Remember our rule of thumb: For a gaussian \( \gaussian \left( \mathbf{\mu}, \mathbf{\Sigma} \right) \), 
        any time we apply a linear operation on the mean (eg. \( \mathbf{A} \mu \) ), 
        the variance will get that same operation applied twice (eg. \( \mathbf{A} \mathbf{\Sigma} \mathbf{A}^T \) ).
    </p>

    <p>
        Using this rule, we can easily write aout the variance:
        $$ \stateCovariance_{\discreteTime} = \left( \identityMatrix - \kalmanGain \measurementMatrix \right) \stateCovariance \left( \identityMatrix - \kalmanGain \measurementMatrix \right)^T + \kalmanGain \measurementNoiseCovariance \kalmanGain^T $$
    </p>

    <p> Option 2:
        <ul>
            <li>The state you Predicted: \( \gaussian \left( \state, \space \stateCovariance \right) \)</li>
            <li>The state implied by the measurement: \( \gaussian \left( \measurementMatrix^{-1} \measurement, \space \space \measurementMatrix^{-1} \measurementNoiseCovariance (\measurementMatrix^{-1})^T \right) \)</li>
        </ul>
    </p>

    <p>
        \( \text{The Variance Ratio, in state space } \varianceRatio = \frac{\stateCovariance}{\stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance (\measurementMatrix^{-1})^T} \)
    </p>

    <p>
        \( \state = (\identityMatrix - \varianceRatio) \state + \varianceRatio \measurementMatrix^{-1} \measurement \)
    </p>

    <p>
        Bam!! Just like that we've derived the Kalman Filter! 
    </p>
    <p> 
        There's only one problem: \(\measurementMatrix\) is not necessarily square, so it cannot be inverted. 
    </p>
    <p>
        This was actually how I first went about trying to understand the Kalman Filter, and how my feelings were:
        <img src="./assets/Grus-plan-Measurement-Matrix-Not-Invertible.png" alt="Gru's Plan meme: Measurement Matrix is not invertible" style="max-width: 100%; height: auto;">

    </p>

    <p> 
        Fortunately, there is a workaround! If we suspend our disbelief and do a bit of matrix algebra as if \( \measurementMatrix^{-1} \) exists,
         we can avoid ever having to invert the measurement matrix. We'll use the following identities:
        For any invertible matrices \(\mathbf{A}\), \(\mathbf{B}\): 
        \( (\mathbf{A} \mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1} \), 
        \( (\mathbf{A}^{-1})^{-T} = (\mathbf{A}^T)^{-1} \), 
        and we can always multiply anywhere by the identity \( \identityMatrix = \measurementMatrix^{-1} \measurementMatrix = \measurementMatrix^T (\measurementMatrix^T)^{-1} \)
    </p>

    <p>
        So let's knock out those inverses!
        $$ \begin{align*}
        \varianceRatioState &= \stateCovariance \left[ \stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right]^{-1} \\
        &= \stateCovariance \left[ \stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right]^{-1} \left( \measurementMatrix^{-1} \measurementMatrix \right)\\
        &= \stateCovariance \left[ \measurementMatrix \left( \stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right) \right]^{-1} \measurementMatrix \\
        &= \stateCovariance \left[ \measurementMatrix \stateCovariance + \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right]^{-1} \measurementMatrix \\
        &= \stateCovariance \left( \measurementMatrix^T \left( \measurementMatrix^T \right)^{-1} \right) \left[ \measurementMatrix \stateCovariance + \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right]^{-1} \measurementMatrix \\
        &= \stateCovariance \measurementMatrix^T \left[ \left( \measurementMatrix \stateCovariance + \measurementNoiseCovariance \left( \measurementMatrix^{-1} \right)^T \right) \measurementMatrix^T \right]^{-1} \measurementMatrix \\
        \varianceRatioState &= \stateCovariance \measurementMatrix^T \left[ \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right]^{-1} \measurementMatrix \\
        \end{align*}
        $$
    </p>
    <p> Now our Variance ratio is free of \( \measurementMatrix^{-1} \), but there is still one 
        more remaining in the weighted sum formula. This can be easily disappeared with the 
        rightmost \(\measurementMatrix\) in the formula, so we'll write our variance ratio in two parts:
        $$ \begin{align*}
        \varianceRatioState &= \stateCovariance \measurementMatrix^T \left[ \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right]^{-1} \measurementMatrix \\
        \varianceRatioState &= \kalmanGain \measurementMatrix \\
        \end{align*}
        $$
        Where we've defined the true kalman gain as:
        $$ \kalmanGain = \stateCovariance \measurementMatrix^T \left( \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right)^{-1} $$
    </p>
    <p>
        
    </p>



    <p>
    Now we have the Kalman Gain:

    $$ \kalmanGain = \stateCovariance \measurementMatrix^T \left( \measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance \right)^{-1} $$

    Intuitively, you can interpret the Kalman Gain as a process with two steps: Multiplying by the variance ratio, then converting to the space of states (or vice versa if you read the bonus derivation).
        TODO: Make a diagram here, either just labeling the terms, or a whole flow diagram.
    $$ \kalmanGain = \measurementMatrix^{-1} \frac{\measurementMatrix \stateCovariance \measurementMatrix^T}{\measurementMatrix \stateCovariance \measurementMatrix^T + \measurementNoiseCovariance}
    = \frac{\stateCovariance}{\stateCovariance + \measurementMatrix^{-1} \measurementNoiseCovariance \measurementMatrix^{-T}} \measurementMatrix^{-1} $$
    </p>

    <h2>The Full Algorithm</h2>
    <h3>Eg. What you actually need to implement</h3>
    Now we have all the pieces to write out the full algorithm, but we'll make a few adjustments to make it more numberically stable
    <ul>
        <li>(I-KH) can be numerically unstable, so we distribute it out to get</li>
        <li>All of the matrices (F,B,H,etc.) can vary with time, so we'll subscript them with the current timestep</li>
        <li>You should be calling the Predict function as often as you can (the dynamics never stops so neither should your blind estimate),
            but the update function can only be called when there is a new measurement. If your
            sensor is fast, you might always call Predict immediately followed by Update, 
            but if your sensor is slow, 5 predict steps might go by with no update. 
            I've accounted 
        </li>
    </ul>

    <table>
        <tr>
            <th>Predict(\( \controlInput_{\discreteTime}, \space \state_{\discreteTime-1}, \space \stateCovariance_{\discreteTime-1} \))</th>
            <th>Update(\( \measurement_{\discreteTime}, \space \state_{\discreteTime-1}, \space \stateCovariance_{\discreteTime-1} \))</th>
        </tr>
        <tr>
            <th>    
                $$ \state_{\discreteTime} = \stateTransition \state_{\discreteTime-1} + \controlMatrix \controlInput_{\discreteTime} $$
                $$ \stateCovariance_{\discreteTime} = \stateTransition \stateCovariance_{\discreteTime-1} \stateTransition^T + \processNoiseCovariance $$
            </th>
            <th>    
                $$ \kalmanGain = \stateCovariance_{\discreteTime-1} \measurementMatrix^T \left( \measurementMatrix \stateCovariance_{\discreteTime-1} \measurementMatrix^T + \measurementNoiseCovariance \right)^{-1} $$
                $$ \state_{\discreteTime} = \state_{\discreteTime-1} + \kalmanGain \left( \measurement_{\discreteTime} - \measurementMatrix \state_{\discreteTime-1} \right) $$
                $$ \stateCovariance_{\discreteTime} = \stateCovariance_{\discreteTime-1} - \kalmanGain \measurementMatrix \stateCovariance_{\discreteTime-1} $$
            </th>
        </tr>
    </table>

    <h2>Summary</h2>
    <p>
        The <strong>Kalman Filter</strong> is an algorithmic blueprint for incorporating different sources 
        of information into a unified probailistic state estimate. It outlines two ways to 
        propagate the current state estimate to the next:
        <ol>
            <li>Predict: Incorporate information about the default dynamics of the system</li>
            <li>Update: Fuse sensor information with the most recent state estimate 
                using the Kalman Gain</li>
        </ol>
        The <strong>Kalman Gain</strong> is an operator that takes vectors in the 
        measurement space as input, and outputs the weighted contribution of that measurement 
        vector to the total state estimate. This operation can be interpreted as having two 
        steps (in either order):
        <ul>
            <li>Convert from the space of measurements to the space of states</li>
            <li>Weight the contribution of the measurement to the state estimate in proportion
                to the inverse of the measurement's variance</li>
        </ul>
        Note that in this interpretation, the inverse measurement matrix <span id="measurement-matrix-inverse"></span>
        is imagined to convert back from the space of measurements to the space of states, even though
        in practice it is <em>almost never invertible</em>. This is why the Kalman Filter is usually 
        defined without it. 
    </p>
    <p>
        The Kalman Gain has been proven to be the Best Linear Unbiased Estimator (BLUE) for 
        gaussian random variables.
    </p>
    <p>Hopefully while playing you realized that 1) it's not magic, and 2) there are many obvious shortcomings. For example, 
        What if your model isn't linear? What if you don't know the covariance matrix for your sensors? How do small changes 
        in parameters like the timestep affect the filter's behavior? The key takeaway is the underlying principles: 
        how to combine and propagate gaussian random variables.
    </p>

    <h2>Model Summary</h2>
    I've just thrown you 15 variables -- that can be a lot to keep track of! I've tabulated all of them here,
    but throughout the article you can also click on any variable to see its name, and keep clicking to get more information.
    You can also choose the naming convention using the dropdown menu above, which should make it easier to learn across multiple sources.

    Link to Visually Explained:https://www.youtube.com/watch?v=IFeCIbljreY
    Link to Steve Brunton: https://www.youtube.com/watch?v=s_9InuQAx-g

    TODO: Make the highilght colors match the diagram colors

    What made me care?
        Powerful way of combining information in the face of uncertainty
        Elegant Combination of linear algebra and probability theory,
        Lightweight tool used for pretty much any system that are continuously changing
        Used everywhere there is time-series data: Target tracking, navigation, control, robotics, neuroscience, finance
        
    <h2>Appendix</h2>
    Many texts make the distinction between the estimate of the state and the <em>true</em> state. Why don't I?
    I did this because when I was learning, I found that with some effort I could distinguish between 
    \( \state \) and \( \hat{\state} \) and \( \bar{\state} \), as well as terms like "a priori" 
    ( \(x_{\discreteTime | \discreteTime-1} \)) and "a posteriori" ( \(x_{\discreteTime | \discreteTime} \)).
    but ultimately those distinctions did not speed my learning process or aid my intuition.

    If you distinguish between estimated state and true state, then you have to be able to think in 3 layers:
    true reality, your model of reality, and then your algorithm, which tracks your model of reality. 

    From the perspective of someone who wants to gain an intuition for how the filter works and possibly implement one 
    (but not necessarily rigorously prove its optimality in some theoretical environment), the distinction
    between model and algorithm is just extra fluff. In practice there are only two processes happening: reality, 
    and the algorithm that tracks reality. When learning and implementing, it is easier to treat the algorithm as 
    both theoretical model and algorithm, and simplify the notation. We admit that our theoretical model of 
    reality is wrong (eg. the assumption of linearity and gaussian noise), but we can simply add these to the 
    list of things that make our algorithm differ from reality (like choosing a timestep and noise parameters).

    I will be the first to admit that I know relatively little about rigorously proving the optimality of the Kalman Filter
    in a theoretical context (despite having sat through lectures that proved it). I think it may be possible to 
    rigorously unite the algorithm with the model (and by that I mean stop distinguishing between estimate and true state), 
    but, not having done so, I cannot say that that distinction is useless in general.    
</body>
</html>